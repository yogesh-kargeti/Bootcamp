Session: Hadoop Setup & Administration
Assignment

Question1. 
Let's assume that you have 100 TB of data to store and process with Hadoop.
The configuration of each available DataNode is as follows:
8 GB RAM
10 TB HDD
100 MB/s read-write speed

You have a Hadoop Cluster with replication factor = 3 and block size = 64 MB.

In this case, the number of DataNodes required to store would be:
Total amount of Data * Replication Factor / Disk Space available on each DataNode
100 * 3 / 10
30 DataNodes

Now, let's assume you need to process this 100 TB of data using MapReduce.

And, reading 100 TB data at a speed of 100 MB/s using only 1 node would take:
Total data / Read-write speed
100 * 1024 * 1024 / 100
1048576 seconds
291.27 hours

So, with 30 DataNodes you would be able to finish this MapReduce job in:
291.27 / 30
9.70 hours

1.Problem Statement
How many such Data Nodes would you need to read 100TB data in 5 minutes in your Hadoop Cluster?

Ans.
Time taken by 1 node to read 100TB data at a speed of 100MB/s= 291.27 hours = 17476.2 minutes

So, no of data nodes needed to read 100 TB of data in 5 minutes in hadoop cluster:
17476.2/5
3495.24 data nodes

S0, we will need 3495 Data Nodes to read 100TB data in 5 minutes in our Hadoop Cluster.
